\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Learning Relative Interactions through Imitation*\\
{\footnotesize \textsuperscript{*}Project for Robotics Course - Academic Year 2019/2020
}
\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{Giorgia Adorni}
\IEEEauthorblockA{\textit{Università della Svizzera Italiana} \\
\textit{Faculty of Informatics}\\
Lugano, Switzerlad \\
(giorgia.adorni@usi.ch)}
\and
\IEEEauthorblockN{Elia Cereda}
\IEEEauthorblockA{\textit{Università della Svizzera Italiana} \\
	\textit{Faculty of Informatics}\\
Lugano, Switzerlad \\
(elia.cereda@usi.ch)}}

\maketitle

\begin{abstract}
The main objective of this project is learning to perform specific interactions between the robot and objects in the 
environment.
Write an omniscient controller that positions the marXbot relatively to a simple horseshoe-shaped object. Use it to 
generate a dataset and train a model that imitates it.
\end{abstract}

\begin{IEEEkeywords}
component, formatting, style, styling, insert
\end{IEEEkeywords}

\section{Introduction}
Defined the marXbot \cite{bonani2010marxbot} as the target platform, whose main characteristic is a rotating laser 
scanner, which perceives distances and colours of objects surrounding the robot. The experiments are run in Enki  
\cite{enki}, a high-performance open-source simulator for planar robots. Enki provides collision and limited physics 
support for robots evolving on a flat surface. 
Moreover, it can simulate groups of robots hundreds of times faster than real-time.

The main objective of this project is learning to perform specific interactions between the robot and objects in the 
environment.
Write an omniscient controller that performs the desired interaction with complete knowledge of the environment (e.g. 
position the robot at a certain location relative to an object) using Enki.
Generate a dataset of simulation runs through Enki. 
Through imitation learning, train an end-to-end neural network that receives as inputs the sensor distances and the 
camera image readings and produces commands for the motors that are the left and the right wheel target speeds.
Evaluate the model trained using Enki.
See \cite{pitch} for a brief pitch of the project. 


\section{Controller}

\subsection{The omniscient controller}

The robot is moved following an optimal “omniscient” controller \cite{park2011smooth}. This controller has complete 
knowledge of the environment. Using the known poses of the robot and of the target object, the omniscient control moves 
the robot to the goal pose, as fast as possible. In particular, the controller checks if the euclidean distance from 
the goal is less than 1mm and if the robot orientation is less than 0.5 degrees from the goal orientation.

\section{Data Generation Through Enki Simulations}
Using Enki, a dataset containing 2000 simulation runs is generated. Each run differs in the initial position of the 
robot. In particular, each of these sets up a world with:
\begin{itemize}
	\item a horseshoe-shaped object, that represents a hypothetical docking station, always in the pose $(x=0, y=0, 
	\theta=0)$
	\item a marXbot, positioned randomly around the object, up to a maximum distance. The goal pose is in front of the 
	two arms of the object.
\end{itemize}

The dataset records the run as sequence of time-steps containing the following information: 
\begin{itemize}
	\item run
	\item step
	\item name
	\item initial position (axis: x, y) 
	\item initial angle
	\item goal position (axis: x, y)
	\item goal angle
	\item position (axis: x, y) 
	\item angle
	\item wheel target speeds (wheel: l, r)
	\item scanner distances (scanner angle), 
	\item scanner image": (scanner angle, channel: r, g, b), 
	\item goal reached 
	\item goal position distance
	\item goal angle distance
\end{itemize}
The run is stopped if the robot reaches the target or, in any case, after 20 seconds, that means that each run can 
contain a maximum of 200 time-steps.

\subsection{Dataset visualisations}
%\begin{figure}[htbp]
%	\centerline{\includegraphics[width=.5\textwidth]{../datasets/omniscient/images/10-robot-trajectories.pdf}}
%	\caption{Trajectories of ten randomly selected runs.}
%	\label{fig:trajectories-omniscient}
%\end{figure}
%
%\begin{figure}[htbp]
%	\centerline{\includegraphics[width=.5\textwidth]{../datasets/omniscient/images/positions-heatmap.pdf}}
%	\caption{Density of samples in each location.}
%	\label{fig:densisy-omniscient}
%\end{figure}
%
%\begin{figure}[htbp]
%	\centerline{\includegraphics[width=.5\textwidth]{../datasets/omniscient/images/distances-from-goal.pdf}}
%	\caption{Distance from goal over time.}
%	\label{fig:distance-from-goal-omniscient}
%\end{figure}
%
%\begin{figure}[htbp]
%	\centerline{\includegraphics[width=.5\textwidth]{../datasets/omniscient/images/goal-reached.pdf}}
%	\caption{Distribution of the reached goals over time.}
%	\label{fig:goal-reached-omniscient}
%\end{figure}
%
%\begin{figure}[htbp]
%	\centerline{\includegraphics[width=.5\textwidth]{../datasets/omniscient/images/initial-final-positions.pdf}}
%	\caption{Initial and final positions of the robot.}
%	\label{fig:initial-final-positions-omniscient}
%\end{figure}

%Laser scanner readings and control signals over time

\section{Proposed Model}
The original dataset, that is the set of all the runs, is then shuffled and split, based on the single run, into the 
train, the validation and the test sets. The proportions are 70\% for training and 15\% each for validation and 
testing. In total, the training set is composed of 187000 samples.
The network trained is a CNN,  which takes as inputs the sensor distances and the camera image readings and as output 
produces the left and the right wheel target speeds. 
The input size is 180x4. The model is trained for 313 epochs, until early stopping interrupts it. 
The training set data are shuffled in each epoch, so the mini-batches (that are of size $2^14$) are generated 
independently. 
Adam is used as optimiser with learning rate $0.01$ and the loss function chosen is the Mean Squared Error (MSE). The 
other parameters have their default values. 
In the network the ReLU non-linearity is applied after every layer except the last one. 
The structure is the following:

\begin{table}[htbp]
	\caption{Architecture of the Baseline Network}
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|}
			\hline
			\textbf{Layer}&\textbf{Channels} &\textbf{Kernel size} &\textbf{Stride} &\textbf{Padding}\\
			\cline{1-5}
			conv1 &  4 $\rightarrow$ 16 & 5 & 2 & 2, circular \\ \hline
			conv2 & 16 $\rightarrow$ 32 & 5 & 2 & 2, circular \\ \hline
			conv3 & 32 $\rightarrow$  			 32 & 5 & 1 & 2, circular \\ \hline
			fc1 &   45 $\times$ 32 $\rightarrow$ 128 &  &  &  \\ \hline
			fc2 &  128 $\rightarrow$ 128 &  &  &  \\ \hline
			fc3 &  128 $\rightarrow$   2 &  &  &  \\ \hline
		\end{tabular}
		\label{tab: baseline}
	\end{center}
\end{table}

\begin{table}[htbp]
	\caption{Architecture of the Network with Max Pooling}
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|}
			\hline
			\textbf{Layer}&\textbf{Channels} &\textbf{Kernel size} &\textbf{Stride} &\textbf{Padding}\\
			\cline{1-5}
			conv1  &   4 $\rightarrow$  \bfseries	32 & 5 & 2 & 2, circular \\ \hline
			conv2  & \bfseries 32 $\rightarrow$  	96 & 5 & 2 & 2, circular \\ \hline
			\bfseries mpool1 & 					   & \bfseries 3	& \bfseries 3 & \bfseries 1, circular \\ 
			\hline			
			conv3  & \bfseries 96 $\rightarrow$  	96 & 5 & 1 & 2, circular \\ \hline
			fc1    & 15 $\times$ 96 $\rightarrow$ 128 &  &  &  \\ \hline
			fc2    & 128 $\rightarrow$ 128 &  &  &  \\ \hline
			fc3    & 128 $\rightarrow$   2 &  &  &  \\ \hline
			%\multicolumn{5}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
		\end{tabular}
		\label{tab: maxpool}
	\end{center}
\end{table}

\begin{table}[htbp]
	\caption{Architecture of the Network with Dropout}
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|}
			\hline
			\textbf{Layer}&\textbf{Channels} &\textbf{Kernel size} &\textbf{Stride} &\textbf{Padding}\\
			\cline{1-5}
			\multicolumn{5}{|c|}{...} \\ \hline
			fc1 &  1440 $\rightarrow$ 128 &  &  &  \\ \hline
			\bfseries drop1 & \multicolumn{4}{c|}{\bfseries dropout with p = 0.5} \\ \hline
			fc2 &  128 $\rightarrow$ 128 &  &  &  \\ \hline
			\bfseries drop2 & \multicolumn{4}{c|}{\bfseries dropout with p = 0.5} \\ \hline
			fc3 &  128 $\rightarrow$   2 &  &  &  \\ \hline
			%\multicolumn{5}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
		\end{tabular}
		\label{tab: dropout + mpool}
	\end{center}
\end{table}



\begin{table}[htbp]
	\caption{Architecture of the Network for Task 2}
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|}
			\hline
			\textbf{Layer}&\textbf{Channels} &\textbf{Kernel size} &\textbf{Stride} &\textbf{Padding}\\
			\cline{1-5}
			conv1  &  4 $\rightarrow$ 	32 & 5 & 2 & 2, circular \\ \hline
			conv2  & 32 $\rightarrow$  	96 & 5 & 2 & 2, circular \\ \hline
			mpool1 & 					   & 3	& 3 & 1, circular \\ 
			\hline			
			conv3  & 96 $\rightarrow$  	96 & 5 & 1 & 2, circular \\ \hline
			fc1   &  1440 \textbf{+ 3} $\rightarrow$ 128 &  &  &  \\ \hline
			drop1 & \multicolumn{4}{c|}{dropout with p = 0.5} \\ \hline
			fc2   &  128 $\rightarrow$ 128 &  &  &  \\ \hline
			 drop2 & \multicolumn{4}{c|}{ dropout with p = 0.5} \\ \hline
			fc3 &  128 $\rightarrow$   2 &  &  &  \\ \hline
			%\multicolumn{5}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
		\end{tabular}
		\label{tab: task 2}
	\end{center}
\end{table}


\subsection{Model performance}
%\begin{figure}[htbp]
%	\centerline{\includegraphics[width=.5\textwidth]{../models/net6/images/initial-positions.pdf}}
%	\caption{Initial positions divided by belonging dataset.}
%	\label{fig:initial-positions}
%\end{figure}
%
%\begin{figure}[htbp]
%	\centerline{\includegraphics[width=.5\textwidth]{../models/net6/images/loss.pdf}}
%	\caption{Comparison of the losses among the train and validation sets.}
%	\label{fig:loss}
%\end{figure}
%
%\begin{figure}[htbp]
%	\centerline{\includegraphics[width=.5\textwidth]{../models/net6/images/distribution-target.pdf}}
%	\caption{Comparison of the distributions of groundtruth and prediction of the validation set.}
%	\label{fig:distribution-target}
%\end{figure}
%
%\begin{figure}[htbp]
%	\centerline{\includegraphics[width=.5\textwidth]{../models/net6/images/regression.pdf}}
%	\caption{Comparison of the $R^2$regressor between groundtruth and prediction of the validation set.}
%	\label{fig:regression}
%\end{figure}

\section{Experiments}
Using the learned controller, a new dataset has been generated, in which each run is stopped after 20 seconds, (maximum 
of 200 timesteps). 
In the following figures are shown some visualisation explaining the behavior of the robot controlled by the model.

\subsection{Results}
%\begin{figure}[htbp]
%	\centerline{\includegraphics[width=.5\textwidth]{../datasets/learned/images/10-robot-trajectories.pdf}}
%	\caption{Trajectories of ten randomly selected runs.}
%	\label{fig:trajectories-learned}
%\end{figure}
%
%\begin{figure}[htbp]
%	\centerline{\includegraphics[width=.5\textwidth]{../datasets/learned/images/positions-heatmap.pdf}}
%	\caption{Density of samples in each location.}
%	\label{fig:densisy-learned}
%\end{figure}
%
%\begin{figure}[htbp]
%	\centerline{\includegraphics[width=.5\textwidth]{../datasets/learned/images/distances-from-goal.pdf}}
%	\caption{Distance from goal over time.}
%	\label{fig:distance-from-goal-learned}
%\end{figure}
%
%\begin{figure}[htbp]
%	\centerline{\includegraphics[width=.5\textwidth]{../datasets/learned/images/goal-reached.pdf}}
%	\caption{Distribution of the reached goals over time.}
%	\label{fig:goal-reached-learned}
%\end{figure}
%
%\begin{figure}[htbp]
%	\centerline{\includegraphics[width=.5\textwidth]{../datasets/learned/images/initial-final-positions.pdf}}
%	\caption{Initial and final positions of the robot.}
%	\label{fig:initial-final-positions-learned}
%\end{figure}

\section{Future works/Problems/Solutions}
Even if, after 15-20 epochs the train and validation MSE settles around 40 and does not decrease much afterwards, the 
network seems to be learning. Instead, the dataset can be improved since the network is not able to generalise in such 
cases in which the robot is close to the object or within his arms. This is in fact because of the omniscient 
controller that never gets the robot so near the object, and for the network this is an unseen situation for the 
network.
A solution could be to add some runs in the dataset to explore some peculiar cases where the initial position of the 
robot is near the object or the target position.
Another improvement could be to use different shapes for the object, so that the behaviour that the network should 
learn, that is understanding where the shape entrance is, will be clearer.
%
%\subsection{Abbreviations and Acronyms}\label{AA}
%ReLU
%CNN
%Mean Squared Error (MSE)
%
%
%\subsection{Equations}
%
%\begin{equation}
%a+b=\gamma\label{eq}
%\end{equation}
%
%Use ``\eqref{eq}'' in the middle of the sentence or ``Equation \eqref{eq} is . . .'' at the beginning of a sentence. 
%Use (e.g., \verb|\eqref{Eq}|) cross references.
%
%\subsection{Figures and Tables}
%\paragraph{Positioning Figures and Tables} Place figures and tables at the top and 
%bottom of columns. Avoid placing them in the middle of columns. Large 
%figures and tables may span across both columns. Figure captions should be 
%below the figures; table heads should appear above the tables. Insert 
%figures and tables after they are cited in the text. Use the abbreviation 
%``Fig.~\ref{fig}'', even at the beginning of a sentence.
%


\section*{Acknowledgement}


\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv, biblio}

\end{document}
